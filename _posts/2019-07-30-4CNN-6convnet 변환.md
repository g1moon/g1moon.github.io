---
layout: post
title: "4_CNN_6.cnn 신경망 변환"
subtitle: "4_CNN_6.cnn 신경망 변환"
categories: data
tags: deeplearningai
comments: true
---


# 6.cnn신경망 변환

# C4W4L07 What are deep CNs learning?

- 알렉스 넷이다
- 훈련 세트가 신경망을 거치도록 하고 → 어떤 이미지가 그 유닛의 활성값을 최대화 하는지
- 1층의 히든 유닛은 신경망의 비교적 작은 부분만 본다
- 즉 앞단 층의 유닛일수록 이미지의 비교적 작은 부분을 본다
- 그리고 뒤에 있는 층의 유닛일수록 이미지의 비교적 큰 부분을 보게된다.

![](https://i.imgur.com/m1LmLCl.png)

- 위 그림을 보면 1층의 첫번째 히든 유닛은 사선을 보는 것 같아
- 그리고 1층의 다른 히든 유닛을 골라서 이 과정을 반복하는데
- 9개의 또 다른 유닛을 보면 왼쪽 사선을 의미하는 거 같고, 또 옆에는 초록색의 선을 선호하는 것 같아.,
그리고 아래는 주황색을 선호하고 .... 주황의 동그라미 부분은 오랜지와 그린이 같이있어 다른 색을 만들어
- 이런식으로 다른 9개의 다른 뉴련을 표현해
- 각각의 뉴런에대해 활성화 값을 최대로 만드는 9개의 이미지 조각
- 첫 단의 유닛들은 비교적 간단한 것들을 찾는다

- 깊은 층의 유닛은 이미지의 더 큰 부분을 볼 것
- 하지만 여기서는 똑같은 사이즈로 표기하겠음(이지미 조각들을)
- 맨 끝에서는 각 픽셀들이 가상적으로 신경망의 이후 층들의 결과값에 영향을 미칠 것

![](https://i.imgur.com/zDLSkAf.png)

- 이것들이 신경망을 극도로 활성화 시키는 각 층의 히든 유닛들임
- 첫번쨰 유닛은 이런 각도로 모서리가 있는 이미지 영역이 있는 경우 → 고도로 activate
- 두번쨰는 더 복잡한 것들이 보임 (많은 세로 선을 찾는 것 같고. 원형을 찾는 거 같아, 그리고 아주 얆은 세로선)
- 두번째층이 감지하는 특징이 더 디테일하고 섬세해

![](https://i.imgur.com/iQQvzuU.png)

- 3층을 보면 재미있는데 이미지 왼쪽 아래부분에 동그라미가 있다고하는 것에 크게 반응하니
- 자동차들을 찾아
- 그리고 첫번쨰를 보면 벌집 모양을 찾는 거 같아

![](https://i.imgur.com/GTBg7eg.png)

- 더 디테일하게 개들이 다들 비슷하게 생겼어 , 물을 디텍트하는 거 같기도하구
- 개 검출기로 보이는 뉴런들도 있고

- 예를 들어 1층에서는 모서리 , 2층에서는 텍스처 , 3층에서는 자동차...점점 디테일해짐
- 이게 신경망의 얕은 층과 깊은 층들이 무엇을 계산하는지에 대한 시각화

 

- 참고자료

Visualizing and Understanding Convolutional Networks

[https://www.notion.so/g1moon/6-cnn-82308f6ae4ec46709166b0e7d459ba78#83034898eaa64b9cb637f90a48fb3e5f](https://www.notion.so/g1moon/6-cnn-82308f6ae4ec46709166b0e7d459ba78#83034898eaa64b9cb637f90a48fb3e5f)

## C4W4L08 Cost Function

- 생성 이미지에 대한 cost function을 정의해보자

![](https://i.imgur.com/diMDALY.png)

- c와 s의 이미지로 새로운 g의 이미지를 만드는데
- 신경망 변화를 통해 할 때
- g에 대한 cost function J를 정의한다 → J(G)
- 이 이미지를 생성하기위해 경사 하강으로 J(G)를 최소화 할 것
- 비용 함수의 두가지 부분을 정의하는데
- 첫번쨰 부분은 content  내용에 관한 (내용이 얼마나 비슷한가)
- 두번째 부분은 style 스타일이 얼마나 비슷한지
- 마지막으로 알파와,베타 하이퍼 파라미터를 이용해 → content와 style cost 사이의 weight을 둔다
- 두개의 파라미터는 너무 장황해 보이지만 → 관습에 따라 그냥 사용

![](https://i.imgur.com/jOyYJCN.png)

- 비용함수가 정의되었을때 !
- 1 생성 이미지(G)를 랜덤하게 정의해 (차원도 아무렇게)-아마도 rgb채널 이미지
- 2.그런 다음에 J(G)를 정의하고 이것을 최소화 시켜 (경사 하강으로)

![](https://i.imgur.com/5BurN0X.png)

예시로)

- 이 content image, style image 에서 시작
- G를 랜덤하게 초기화 시키면 → 초기 생성 이미지는 → 각 픽셀 값이 무작위로 선택된 저런 흰색 노이즈일 것
- 그리고 그라디언트 디센트로 미니마이즈하면 피셀갑시 점점 내용 이미지와 비슷하지만 , 스타일 이미지의 스타일인 이미지를 갖게 된다

## C4W4L09 Content Cost Function

![](https://i.imgur.com/4ESk4nk.png)

- 위의 식이 생성이미지에 대한 코스트 펑션
- 1. content cost function을 계산하기위해 hidden layer l층을 사용한다고 해보자
- 만약 l이 매우 작다면 (1층)
- 내용 이미지와 아주 유사한 이미지를 생성하도록 할 것
- 아주 깊은 층을 사용하면 이미지에 개가 있을 때 생성 이미지의 어딘가에 개가 있게된다.
- 실제로 l이 신경망의 너무 얕은 층과 , 깊은층 사이에 있게 해야한다.(보통l은 중간층에서 결정된다)

- 2. 미리 훈련된 convnet을 사용한다
- 두 이미지 c와G에대한 l층의 활성 값으로 둔다 → 이 두 활성 값이 비슷하다면 = 두 이미지가 비슷한 내용을 의미
- 그래서 우리가 해야하는 건 J내용(C,G)를 ==두 활성값(C,G)의 차이로 정의하는 것
- l층 히든 유닛들에 대해서 활성값의 차이를 구할 것임 ( 내용 이미지를 넣었을 때와, 생성 이미지를 넣었을 때를 비교해서 스퀘어를 구하고  → 앞에 nomalization 상수를 두고 (1/2)
- 아래 식은 단지 각 성분들에 대한 활성값 차의 제곱의 합
- 나중에는 G값을 구하기 위해 J(G)에 경사하강을 적용 → 전체 비용이 낮아지면 → 알고리즘이 히든 레이어의 활성값을 내용 이미지와 비슷하게 만드는 이미지 G를 찾도록 할 것임

### 정리

![](https://i.imgur.com/DF5aYCR.png)

### 참고 논문

A Neural Algorithm of Artistic Style

[https://arxiv.org/abs/1508.06576](https://arxiv.org/abs/1508.06576) 

## C4W4L10 Style Cost Function

- 

![](https://i.imgur.com/g9Ck9Nm.png)

![](https://i.imgur.com/e5tLrZV.png)

![](https://i.imgur.com/reqQLW3.png)

- correlation 이 무엇을 의미할까? (상관 관계가 있다는 것이 뭘까)
- l층의 히든 유닛들이 아래와 같이 있을때 박스친 두 곳을 보면
- 하나는 **세로선을** 나타내고, 다른 하나는 **주황색**을 나타낸다.
- 여기서 degree of correlation 이 높게 나온다면 : **세로선을 나타내는 것에 주황색이 많다**는 것
- degree of correlation 이 낮게 나온다면 : 세로선을 나타내는 것에 주황색이 별로 없다는 것
- 즉 세로선 텍스처와 주황색 색조가 얼마나 함께 자주 발생하는지

![](https://i.imgur.com/Y3Et8R3.png)

- 이 직관을 공식화 시키면
- a superscript-[l] subscript-i,j,k를 hidden-layer-l의 위치 i,j,k의 활성 값으로 둔다
- i는 높이 j는 너비 k는 체널 (이전 슬라이드에서는 5개의 채널이 있어서 k =5)

- 여기서 style matrix가 하는 것은 G superscript-[l]의 행렬을 계산하는것 nc x nc dim의 행렬
- 따라서 정사각행렬
- n_c개의 채널이 있다는 것을 기억해야함
- 따라서 각 쌍들이 어떤 상관관계를 갖는지 알기위해 nc x nc dim의 행렬이 필요함
- 특히 G^[l]_kk' 는 채널 k와 k'의 활성값이 어떻게 연관되는지를 측정
- 여기서 k의 범위는 1부터 해당 layer 의 채널 수 (1~5)
- 

![](https://i.imgur.com/HKUtKvP.png)

![](https://i.imgur.com/Y3Et8R3.png)

- 이미지의 높이와 너비를 따라 서로 다른 위치들을 더한 다음 →
- k와 k'에 대한 활성 값을 곱하는 것  → 이것이 gkk'의 정의
- 행렬 g를 계산하기위해 모든 k와 k'd에 대해 이를 반복 → **'style metrix'**

- 서로 연관이 있다면 g kk'은 클것이고 아니라면 작을 것
- 관습적으로 나타내기위해 correlation이라는 term을 사용하지만 실제로 이것은 unnormalized cross covariance (비정규화 교차 공분산) 임 왜냐하면 평균을 빼지 않고 단지 엘레멘츠들을 곱하기 때문에
- 이 스타일 메트릭스를  스타일 이미지와, 생성이미지에 대해 실행
- 여기에 초록색 s를 더하는데 s 이미지에 대한 활성값임을 나타내기위해

![](https://i.imgur.com/kFLNA4Q.png)

이미지 s와 g에 모두 적용

![](https://i.imgur.com/7XdOnfU.png)

![](https://i.imgur.com/HOnhLGz.png)

- 만약 l층에서 s,g에 대해 구한다면 →
- 이것을 두 행렬의 차의 제곱으로 정의하고, 행렬이니까 그냥 프로베니우스 노름을 취함

![](https://i.imgur.com/Naj21EV.png)

![](https://i.imgur.com/RIsJw8O.png)

노름을 풀어서 아래처럼 쓰고 저자는 앞에  정규화 상수를 사용했는데 어차피 하이퍼 파라미터 베타랑 곱해져야해서 큰 상관이 없음

![](https://i.imgur.com/t7KayH2.png)

![](https://i.imgur.com/rxNGYOu.png)

## C4W4L11 1D and 3D Generalizations

![](https://i.imgur.com/uXsM0hg.png)

- 3d란 1차원 숫자 리스트나 , 2차원 숫자 배열 대신에 , 이젠 3d블록(3차원 부피의 숫자 입력 값)이 있음

![](https://i.imgur.com/LXlugST.png)

- 데이터가 어떤 높이, 너비, 깊이를 갖는다고 생각하는 것
- 위 박스 부피의 단면들은 몸통의 단면을 뜻함

![](https://i.imgur.com/bRFH9qS.png)

- 맨 처음 아이디어를 3차원 합성곱으로 일반화 함
- 만약 저런 3d부피가 있으면 단순하게 14 14 14 라고 하면 이게 ct 스캔의 높이 너비 깊이임
- 이미지와 마찬가지로 꼭 정육면체일 필요는 없음 (높이와 너비가 다를 수 있는 것 처럼 다를 수 있음)
- 14 14 14 를 5 5 5 로 합성 곱하는데 필터도(5,5,5로 3d임) 합성곱해서
- 결과는 10 10 10 임 그리고 또한 1개의 채널을 가질 수 있음→ 그럼 필터에도 채널 수 가 곱해짐
- 그리고 5 5 5 1 에대한 16개의 필터가 있다면 → 10 10 10 16이 나옴 → 이것이 3d데이터 신경망의 한 층

![](https://i.imgur.com/Jfuk0Uy.png)

- 그리고 다음 층은 이걸 다시 5 5 5 16 필터와 합성곱해 → 16은 마찬가지로 채널 수 → 만약 32개의 필터가 있다면 → 6 6 6 부피에 32개 체널 6 6 6 32 가 나옴

- 따라서 3d 데이터는 3차원 컨브넷으로 바로 학습될 수 있음
- 필터가 하는 일은 3d데이터의 특성을 검출하는 것
- ct스캔은 3d  볼륨의 한 예임 , 다른 예로는 영화 데이터로 생각 할 수 있음
- 각각의 조각들은 시간에 따른 영화의 부분임 이걸 이용해 영화속에 움직임을 검출 할 수 있음
