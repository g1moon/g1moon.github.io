---
layout: post
title: "D. Neural Language Models"
subtitle: "Neural Language Models"
categories: data
tags: lecedwithnlp
comments: true
---
# D. Neural Language Models

# # Autoregressive language modelling

- 시퀀스가 주어졌을때 어떻게 스코어링 할 건가
- 첫번째 토큰이 나올 디스트리뷰션 , 첫번째가 주어졌을떄 그 다음 distribution 어떤지 ........ 이걸 곱하면 conditional distribution
- 각각의 컨디셔널을 보면은 input, output 페어에대해 러닝을 하는 것
- 비지도를 → 지도학습으로
- 센텐스 스코어링하는 거는 사실상 텍스트 클레시피케이션을 여러번 하는 것

### # 모델을 어케 만들까

- x1~xt-1까지 앞부분이있고 → 테이블 룩업 → sentence representation(5개방법) → represent가 나오면 arbitrary sub-그래프를 나가 → softmax
- vocab의 모든 확률이 나오고
- 트레이닝할떄는 그 많은 가능성 중 실제 데이터의 토큰의 확률을 높여주는
- loss펑션을 negative log likelihood 이걸 모든 토큰에 다 하고
- 트레이닝 데이터는 센텐스가 주어주고, → 인풋 아웃풋페어가 있는 슈퍼바이즈드 트레닝 셋으로 바꾸고 → 텍스트 클래스 클레시피케이션 모델 트레이닝

# # Probability in 5 min

- 확률은
- non-negative
- sum하면 1
- Multiple random variables : consider two here **X,Y**
- joint probability : 2개가 각각 동시에 일어날 수 있는 확률
- condition probability : x가 ei일때 y가 ej일 확률
- 
- 

![](https://user-images.githubusercontent.com/44131043/66131184-ec118a00-e62d-11e9-9680-c42bda716c1e.png)

- Marginal probability(한계확률 어떤 단일사상 A가 발생할 확률 즉 p(a)를 말함
- 두개 이상의 변수를 고려할 때 어느 한쪽만의 확률
- joint prob가(두개가 동시에 일어 날 확률) 주어졌을 때, variable 하나는 관심이없음, 어케돼도 상관없음 → joint prob를 쭉 본다음에 한쪽 varible이 일어날 거를 모든 걸 결정해서 다 더해줌
- ex)동전 두개를 던지는데 이 두개가 independent하지 않아 → 어케 던지냐에 따라 head냐 tail이냐 확률이 계속 바껴
- 근데 첫번째꺼는 안 궁금하고 , 계속 던져본 다음에 두번쨰 head로 떨어진게 중요하다고하면 얘가 head로 떨어진거 x모든 거를 보고 모두 더해버림

# # N-Gram Language Models

- 뉴럴넷을 쓰기 이전에 n-gram 언어 모델 학습방법
- 동전을 수억번 던지는 느낌
- 계속해서 이 phrase가 몇 번 나왔나 , 주어진 n그램에서 앞에 n-1개 빼고 나머지가 다 바뀐다해고했을떄 몇번 나왔나 → 첫번쨰꺼를 두버째꺼 나누면 → 최대우도추정(Maximum Likelihood Estimation) 나오고 → n-gram proba를 구할 수 있음
- 새로운 문장이 주어졌을때 앞에 n개 나온거 뒤에 나올 거 , 계싼하고 계싼하고 곱하고 곱하면 확률이 나옴
- 여러 가능성들을 조합해보고 높은 확률을 갖는 거로 한다.

- n개의 프리시딩 토큰이 나온 상태에서 다음에 뭐가 나올지 다 계산해보고 marninal out → 앞에n개의 컨텍스트 토큰만 나오는 경우에 prob가 어케 되는지 알 수 있고 → conditional prob를 계산할 수 있음
- 주어진 n개의 토큰에대해 , 토큰x가 나올 수 있는 확률은
(이전 n개의 토큰)과 (토큰x) 의 joint prob을 
(이전 n개의 토큰들들)과 (등장할수있는모든토큰들의 joint prob합으로) 나눈

![](https://user-images.githubusercontent.com/44131043/66131186-ecaa2080-e62d-11e9-8234-c50daaf73b28.png)

- 동전을 던지는데 동전이 삐뚤어져서 head로 떨어지는 확률을 알고싶어
- 그럼 막 만번 던져보고 몇번 헤드로 떨어젔는지 보고
- 헤드로떨어진개수/만번 → (Maximum Likelihood Estimation)
- 큰 데이터에서 한번 쭉 읽으면서 카운트만 잘하면 잘 작동함
- 이런 아이디어로 책을 다 긁어와서 → 텍스트(데이터)를 만들고 → 쭉 카운팅을 하면서 n-gram들의 개수를 샘
- 첫번쨰부터 가면서 n이 정해지면 n phrase를 보고 카운팅(이 프레이즈 몇번 나왔나) → 이렇게하면 주어진 n-gram에 대해 몇번 나왔나 알 수 있고 →계속해서 이 phrase가 몇 번 나왔나 , 주어진 n그램에서 앞에 n-1개 빼고 나머지가 다 바뀐다해고했을떄 몇번 나왔나 → 첫번쨰꺼를 두버째꺼 나누면 → 최대우도추정(Maximum Likelihood Estimation) 나오고 → n-gram proba를
- 뉴욕대학교, 뉴욕 다리, 뉴욕 도서관, 뉴욕 관광지,뉴욕 대학교..... 이렇게 n-gram을 다 뽑고 → n-gram count하고 → prob 뽑아

- 위 방법에는 문제가 있음.
1. data sparsity issue
- 라마랑 사자가 평상시에 다른 곳에 사니까 , chasing a llama인데 이전에 이런 정보가 없었기에 확률은 0이 된다.
2. long-term dependencies를 캡쳐 할 수 없음
- n이 fix인데 n이 커지면 커질수록 data sparsity 문제가 커짐
- 모든 단어를 보면 (n이1) → data sparsity issue가 없지만 → probability를 못 잡ㅇ
- long-term dependencies 를 잡으려고 n을 늘리면 ... data sparsity  에휴,,,

- 해결방법:(만족스러운 해결 방법은 아
1. 데이터 희소성 문제: 카운트가 0일때 문제 , n은 정해져있고
- Smoothing: add a small constant to avoid 0 (상수항을 더해 0의 문제 해결) → 체이싱 어 라마도 0이라서 constant줘서 0.2d인데, 체이싱 유니버시티도 0.2.... → 문제....
- Back off: Count 가 0이 나올시에→ 
측정하는 n을 하나씩 줄여서 다시 Count합니다.(n-1 gram을 본다)
→ 즉 처음거를 빼고 작은 거를 봐서 작은거의 카운트  곱하기 constant constant → 계속 0이면 → 계속 줄여가

체이싱어 라마가 하나도 안나왔어 → 에이 라마를 대신 보고 걔의 카운터를 보되 → 그 카운터를 그대로 쓰면 → 더해서 1이 안되니까 → 코렉션 백터로 살짝 줄이고 더하고 → 

2. Long Term Dependency issue 
- n-gram probability 방식으로 해결 할 수가 없습니다.
- 직접 구현하지 않고 KenLM 패키지를 사용하는 것이 좋습니다.

# # Neural N-Gram Language Model

- 트레이닝 셋에 없는 N-gram이라도,,,
- n-gram자체가 테스트할때는 나오고 트레이닝에는 안 나온다..
- 어떤 토큰들이 비슷하고 다른지를 알 수 없음 → 스코어(prob)를 계싼 할 수 없음
- 이러한 문제를 뉴럴엔그램은 →
체이싱 어 라마는 한번도 안 나왔지만... 비슷한 n-gram이 많이 나와 
체이싱 어 켓, 체이싱 어 디어 ...비슷한데 비슷한 거만 알 수만 있다면 →
체이싱 어 라마도 이런 것 들과 비슷하지 않을까?
- 뉴럴넷이 게스하는 방법은 token representaion보고 sentence representaion보고 → representaion 자체가 continuous vector space에 있기 때문에 → 비슷한 애들은 가까이 붙고, 먼 애들은 떨어져., 
→ 처음 봐도 그럼 비슷한 phrase는 비슷하게 붙어 있고 → 비슷한 distribution을 갖는다

![](https://user-images.githubusercontent.com/44131043/66131185-ec118a00-e62d-11e9-9865-2eafb9883570.png)

- 3개의 문장이 있을때 이 예제에서 대부분의 카운트는 0이될 것
- bigram phrase를 보면 테스트하는 타임에 three 컨텍스트일때 group의 prob를 구해야하면 → count fix에서는 0
- 뉴럴넷으로하면? 트레이닝하면서
- three teams가 포인트로 매핑되고 → 그 매핑된 포인트에서 teams의 prob를 높아지도록 만들겠지
- 그러다가 four teams라는 바이그램 → four가 input, output에서 teams의 log prob가 높게 나오는데
- three 가되든 four가되든 그 다음에 따라오는 토큰이 teams다 그래서 → teams의 prob를 높여햐한다고 생각하고 트레이닝하면 → **optimal behavior은 three와 four을 하나의 포인트로 매핑하는게 옵티멀한 방법임**
- 비슷하다 비슷하지 않다의 정의는 : 이 프레이즈가 나온 다음에 → 다음 단어를 프리딕트를 하는데 →  비슷한게 나오느냐 나오지 않느냐 ****
- 쓰리 팀, 포 팀, 포 그룹이 나왔었으니까 → 쓰리 그룹에도 확률을 높게 줘야한다 
→ 이렇게 데이터 희소성 문제 해결 (count 0이 되는 문제)

- In Practice
1. 코퍼스로부터 n gram을 모으고
2. 트레이닝 셋을 만들기위해 순서를 섞어줌 n-grams을 
- stochastic gradient descent에서 알게된게 트레이닝 example을 뽑을 때 최대한 유니폼하게 뽑아야함 → 셔플시킴
3. 앞에서부터 미니배치 사이즈만큼 뽑아서 이 네트워크를 분류기 트레이닝하는 것처럼 트레인 
4. validation set을 이용해 early-stopping하는 것을 잊지 말고(언제 트레이닝을 멈출지) →
5. 테스트셋에서 얼마나 ㅏ잘하는지 리포트해야하는데 perplexity를 리포트
- perplexity이1이면 테스트셋에 있는 단어 보면 다음 토큰 뭐가 나오는지 알 수 있다는 거
- perplexity이 10이면 인풋을 봤을때 그 다음 토큰이 있을 떄 10개 내외에서는 맞출 수 있음
- perplexity가 max는 vocab size와 똑같아질 수 있겠지 →그럼 망함

# # Long Term Dependency

- 뉴럴 엔그램이 정해진 n으로 볼 수 있는게 한계가 있음
- 뉴럴넷을 쓰므로(continouse representation) data sparsity 이슈가 없다보니
- 컨텍스트 사이즈를 막 늘려 그런데 문제점이 생김
- 똑같은 엔그램쓰면서 사이즈 늘리면 아키텍처 사이즈가 커짐
- 그러면 파라미터 개수가 많아지고
- 그럼 데이터셋 사이즈도 같이 커지도(파라미터 에스티메이트를 잘 하려면)
- 그럼 뉴럴 엔 그램 모델로는 한계...

방법으로

- cnn 랭기지 모델
- 로컬로 볼 수 있고
- 계쏙 쌓으면 점점 볼 수 있는게 확대가 되고
- 이중에서도 dilated cnn
- 계속 늘려도 계쏙 로컬하게 보니까 한계가 있음
- 그래서 보는 사이즈를 익스포텐셜하게 늘어나게 할 수 있을까 ? → dilated cnn

![](https://user-images.githubusercontent.com/44131043/66137193-b07bbd80-e637-11e9-97d4-482abe61f6e0.png)

- 가운데거를 보기위해 옆에 두개를 같이보고 , 계속 쌓으면 3개 →5개 →7 그런데 이속도로는 너무 천천히 늘어나 → 너무 많이 쌓아야함
- 그래서 레이어를 쌓을때마다 중간중간 구멍을 뚫어서 가운데를 보기위해 바로 양 옆을 보는게 아니라 → 살짝 떨어져있는 현 위치에 있는에와 두번쨰 앞에있는에 두번째 뒤에있는에

![](https://user-images.githubusercontent.com/44131043/66137191-b07bbd80-e637-11e9-9310-5590654fde5b.png)

- 두번쨰에서는 3개를 봤지만 그 아래에서는 로컬하게 봐서 전부를 봤음 → 빠르게 레이어를 얼마 안 쌓아도 빠르게
- 기존의 랭기지 모델에서는 지금까지 들어온 토큰을 보고 다음 토큰을 예측하는 거였는데
- 현재 단계에서 다음 토큰을 프리딕트를 하는데 이미 다음 토큰을 비롯해 미래의 토큰을 보게됨 → 그러면 확률이 디컴포지션이 안 됨
- 이를 해결하기 위해 매번 프리딕션 할 때마다 미래의 단어를 mask out을 함(레프리젠테이션을 계산 할 때) → 어느 타임에서든 이 레프리젠테이션은 그 전에 나온 토큰만 사용하게 됨
- 컴퓨테이션도 이피시언트해지고
- 파라미터 개수도 너무 빠르게 올라가지 않고
- 컨텍스트 사이즈(N)을 늘릴 수 있었음

![](https://user-images.githubusercontent.com/44131043/66137190-b07bbd80-e637-11e9-8318-cfac4260c189.png)

- 절대 미래의 토큰을 보고 과거의 토큰을 이야기할 수 없음
- 텍스트 클래시피케이션 할때는 과거 미래 구분 없이 , 센텐스 전체 찾을때 레프리젠테이션을 찾는 거였는데 , 랭기지모델, 센텐스 제너레이션 할때는 미래의 토큰을 사용해서 과거의 토큰을 제너레이트하는 문제를 조심해야함

---

- Causal sentence representation

- Convolution Language Model에서 보았듯이, 미래의 토큰을 보지 않고 예측하기 위해 mask out 하는 과정을 거쳤습니다.
- 미래의 토큰을 보지 않고 다음 토큰을 예측하는 과정을 인과관계에 따른 문장표현(Causal sentence representation) 이라고 합니다.
- 이는 인과관계에 따른 문장표현만 지킨다면 문맥을 무한대로 확장가능하다고 볼 수 있습니다.

---

### - Cbow

- 매번 다음 토큰을 프리딕트하기 전에 항상 전에 나온 토큰들을 사용해서 해야함
- 센텐스에서 나온 모든 토큰을 뽑고 → 걔들의 레프리젠테이션을 구해 → ...
- 텍스트 분류에서는 효과적인데 → 랭귀지 모델에는 별로임(의미만아니라 구조 자체가 중요함 → 순서를 무시하는 센텐스 레프리젠테이션은 적합합지 않음)
- 토큰 레프리젠테이션을 잘 찾는다

---

### - rnn(가장 많이 쓰임)

- 앤그램은 롱텀디펜던시를 못 잡으니까
- rnn을 쓰고 많이 사용되고 있음
- 한쪽 방향으로만 가야함 (미래의 토큰을 쓸 수 없으니)

---

### - attention

- 시퀀스가 쭉 주어져있을 때 → 지금까지 본거를 컴프레스해서 → 벡터 하나로 만들어 (rnn의 방법)
- 위 방법은 시퀀스가 짧으면 컴프레스하는데 어렵지 않지만 → 길어지면 컴프레스,기억해야하는 내용이 많아 크게 만들어야하고 → 파라미터 많아지고 → 데이터 커지고 → 악순환
- 어텐션은 매 스텝마다 전에있던 모든 토큰, 그 이후의 모든 토큰에 대해 각각 따로 계산 → 이 뜻은 하나로 컴프레스할 이유가 없음
- 두개 합쳐보자 ~
- rnn돌리고 위에서 prob계산 할때는 self attention 메카니즘 써서 가장 중요한 (rnn을돌려 과거의 토큰을 봤는데)  다음 토큰의 prob를 계산할때 써보자  → Recurrent Memory Networks

---

### - Recurrent Memory Networks

![](https://user-images.githubusercontent.com/44131043/66137194-b1145400-e637-11e9-9c25-913ee379f662.png)

- 이 위치에서 다음번 토큰을 계산하고 싶으면
- rnn으로 계속 요약시키고 → 이 요약한 하나의 벡터만 쓰는게 아니라 이 벡터를 기준으로 전에 있던 모든  벡터들을 다 계싼 → 웨이팅 펑션이 어떤 토큰이 중요했는지 말해주고 → 5개중에서 한개 → 선택이 되면 → 그것만 사용해서 다음번 예측에 사용
- 이렇게 해서 rnn이 지금까지 본 모든 토큰을 압축 시켜 벡터 하나로 넣어야할 일이 없어짐
- 어떻게든 directed acyclic graph 에 들어가기만 하면 작동하므로 → 섞어 쓰는 경우가 많다

# # Summary

- 랭귀지 모델링을 하는게 → 센텐스가 주어지고 →센텐스의 스코어를(확률)구하는건데
- 우리가 본거는 오토리그레시브 랭기지 모델
- 온라인으로 왼쪽→오른쪽 또는 반대로 계산된 스코어를 더하는방식
- 장점 : 비지도학습인데, 위 모델은 이 문제를 클래시피케이션 하는 문제로 바꿔줌
- n-gram 문제를 봤고 2개의 문제
- 뉴럴넷으로 해결하는 방법
- 한번 sentence representation extract하는 방법을 알게 되면 텍스트 분류, 랭귀지 모델링, 머신 트렌슬레이션 퀘스쳔 엔서링 다 할 수 있음
- 스코어를 non-negative하고 sum했을떄 1로 만들어주는것이 → Probability
