---
layout: post
title: "C. Text Classification & Sentence Representation"
subtitle: "Text Classification & Sentence Representation"
categories: data
tags: lecedwithnlp
comments: true
---
# C. Text Classification & Sentence Representation

# # Text Classifiaction

- 텍스트 분류
- 텍스트 분류는 지도학습
- input: 하나의 문장, 문단, 문서
- output: 유일한 c개의 카테고리
- 예시
- 감성분석
- 카테고리 분류
- 의도 분류

# # How to represent sentence & token?

- 재미있는게 텍스트 토큰은 arbitrary인 성격, 주관적 성격을 갖는다
- 왜 개가 개야? dog가 wolf랑 비슷한데 이름은 너무 다르네,,, → arbitrary
- 각 토큰을 vocab에서 몇번 째에 있는지를 해서 바꿔줌
- 각각의 인티저 인덱스를 인코딩할때 arbitrary하다는 것을 encoding 하고싶음
- one-hot encoding(binary vector)→ corresponding하는 것만 1
- 장점으로 아무거나 랜덤하게 2개 뽑아도 그 거리가 모든 토큰의 거리가 모두 같음, 실제로는 one-hot하지 않음
- 비슷한 애들은 가까이 있어야하고, 비슷하지 않으면 멀리 떨어지게
- 뉴럴넷이 각 토큰 하나하나마다 벡터를 갖고 있는다(토큰을 원핫으로 쓰게했으면 원핫 벡터랑 웨잇 메트릭스랑 곱해줌→ 1에 맞는 컬럼이나 로우를 뽑아냄(table lookup) → 임베딩 레이어라고도 말하고~~
- 중요한 문제는 어떻게하면  input size가 계속 바뀌는데, 어떻게하면 fix size representation을 찾을 수 있냐 (문장에대한 의미를 찾을 수 있냐)

---

---

**학습내용**

- 문장은 일련의 토큰(tokens)으로 구성되어 있습니다. 텍스트 토큰은 주관적, 임의적(arbitrary)인 성격을 띄고 있습니다.
- 토큰을 나누는 기준은 다양합니다.
- 공백(White space)
- 형태소(Morphs)
- 어절
- 비트숫자
- 컴퓨터에게 단어를 숫자로 표현하기 위해서, 단어장(Vocabulary)을 만들고, 중복되지 않는 인덱스(index) 로 바꿉니다.
- 궁극적으로 모든 문장을 일련의 정수로 바꿔줍니다. 이를 인코딩(Encoding)이라고 합니다.
- 하지만 관계없는 숫자의 나열로 인코딩하는 것은 우리가 원하는 것이 아닙니다. 여전히 주관적인 숫자들 뿐입니다.
- 우리는 비슷한 의미의 단어는 같이 있고, 아니면 멀리 떨어져 있는 관계를 만들고 싶습니다. 그렇다면 어떻게 관계를 만들어 줘야 할까요?
- 한 가지 방법으로 "One hot Encoding"이 있을 수 있습니다.
- 길이가 단어장의 총 길이(∣*V*∣)인 벡터에서, 단어의 index 위치에 있는 값은 1, 나머지는 0으로 구성합니다.
- *x*=[0,0,0,⋯,0,1,0,⋯,0,0]∈{0,1}​∣*V*∣​​
- 단점: 모든 토큰 간에 거리가 같습니다. 하지만 모든 단어의 뜻이 같지 않기 때문에 거리가 달라져야 저희가 원하는 단어간의 관계가 성립 됩니다.
- 어떻게 신경망이 토큰의 의미를 잡아낼수 있을까요?
- 결론은 각 토큰을 연속 벡터 공간(Continuous vector space) 에 투영하는 방법입니다. 이를 임베딩(Embedding) 이라고도 합니다.
- Table Look Up: 각 one hot encoding 된 토큰에게 벡터를 부여하는 과정입니다. 실질적으로 one hot encoding 벡터( *x* )와 연속 벡터 공간( *W* )을 내적 한 것 입니다.
- Table Look Up 과정을 거친후 모든 문장 토큰은 연속적이고 높은 차원의 벡터로 변합니다.
- *X*=(*e*​1​​,*e*​2​​,⋯,*e*​*T*​​)*where* *e*​*t*​​∈*R*​*d*​​

# # CBoW & RN & CNN

![](https://user-images.githubusercontent.com/44131043/66137865-b2924c00-e638-11e9-9cb1-0dca3a92bc05.png)

1.  CBow (Continuous bag-of-words)
- 순서 무시
- 그냥 에버리지니까 sum이니까 노드 하나임
- 토큰 하나하나만 보면 의미가 덜하니까 두 토큰씩 묶어서 볼 수 있지 않을까?(n-gram)
- 그냥 이게 너무 잘 됨(오더를 무시하고하는데도 잘 됨)

- 토근들이 쭉있는데 각 토큰들을 테이블 룩업레이어(똑같은 weight이으쓰인)로 벡터로 만들고→(seq token → vector)  그것들의 sequence of vector의 avg를 계산 → 거리가 가까우면 비슷한 의미. 멀리 떨어져있으면 반대
- 내가 푸는 문제에 어떤 representation이 적합한지

2.   Relation Network(Skip-Bigram):

![](https://user-images.githubusercontent.com/44131043/66137868-b32ae280-e638-11e9-9a2d-b09645a98c4a.png)

![](https://user-images.githubusercontent.com/44131043/66137866-b32ae280-e638-11e9-9d9d-a43c68d845bd.png)



- 단어나 토큰 두개가 나란히 있는게 bigram인데
- skip bigram이면 토큰 두개를 보는데, skip해서 2개를 봄
- 모든 토큰 페어를 생각함(1,2번째),(1,3),(1,4) 각 페어에대해 뉴럴넷을 만들고
- input이 페어로 들어가고 → 벡터로 바꿔주고 → matmul → ..... > matmul
- 4개 5개 보는 거로도 할 수 있는데(bigram으로 말고) 컴퓨팅 파워가...
- 페어로 했을때 효과가 좋다고들 함
- 모든 나온 벡터터들을 에버리징하고 arbitrary sub-graph에 넣고 softmax
- 페어로 보니까 cbow가 못하는 것을 디텍할 수 있음
- 근데 왜 모든 페어를 봐야할까?( 가까운 단어는 의미있을 거 같은데 첫말이나 마지막 말이 뭔 상관일까...)
- 그래서 relation net의 alternative로 cnn을 볼 수 있음

3. CNN

![](https://user-images.githubusercontent.com/44131043/66137864-b2924c00-e638-11e9-8253-57dda1e9d047.png)

- k gram을 hierachically하게 본다(처음에는 3개를 보고 위에다 더 큰 걸 보고 더 큰 걸보고 → 점점 gradual grow)
- 레이어를 쌓을 때 마다 점짐적으로 넓은 범위를 본다
- 1 dimension vector
- 토큰보고 → continuous vector로 바꾸고 → multi word expression을 보고 더 큰 → phrases를 보고 → sentence를 보고
- 실제 프렉티스에서 컨볼류션넷을 안 쓰는 경우가 거의 없음
- 하드웨어 레벨에서도 구현이 잘 ~, 사용하기 좋음

# # Self Attention & RNN

![](https://user-images.githubusercontent.com/44131043/66137867-b32ae280-e638-11e9-8210-155394dac3cd.png)

- cnn은 아주 먼 거리에 중요한 디펜던시가 있다고하면 → 레이어를 계쏙 쌓아야함,...레이어를 많이 쌓지 않으면 첫 단어와 뒷 단어의 관계를 보기 힘들어
- 필요하면 길게보고, 아니면 짧게ㅔ 보고,,,

### - RN

- 모든 다른 토큰의 관계를 봅니다. 모든 단어간의 관계를 봐서 효율적이지 못합니다.(너무 많이 봐)
- *h*​*t*​​=*f*(*x*​*t*​​,*x*​1​​)+⋯+*f*(*x*​*t*​​,*x*​*t*−1​​)+*f*(*x*​*t*​​,*x*​*t*+1​​)+⋯+*f*(*x*​*t*​​,*x*​*T*​​)

### - CNN:(너무 로컬)

- 작은 범위의 토큰의 관계를 봅니다. 따라서 더 먼 거리의 단어간의 관계가 있을 경우 탐지할 수 없거나 더 많은 convolution 층을 쌓아야합니다.
- *h*​*t*​​=*f*(*x*​*t*​​,*x*​*t*−*k*​​)+⋯+*f*(*x*​*t*​​,*x*​*t*​​)+⋯+*f*(*x*​*t*​​,*x*​*t*+*k*​​)
- 하지만 CNN 방식을 가중치가 부여된 RN의 일종으로 볼 수도 있습니다.
- *h*​*t*​​=∑​*t*​′​​=1​*T*​​*I*(∣*t*​′​​−*t*∣≤*k*)*f*(*x*​*t*​​,*x*​*t*​′​​​​) where *I*(*S*)=1 if *S* is *True* & 0 otherwise

### how to represent a sentence - Self-Attention

- 고정된 0,1로 웨잇을 주는 거 말고 뉴럴 넷이 계산하게 할 수 없을까
- 페어의 웨잇을 계싼하고
- 중요한지 아닌지는 알파 펑션이 정해주고 두개가 관련이 있으면 (뉴와 욕)이건 알파 펑션이 웨잇을 크게 주겠다 (이즈,더)면 웨잇을 작게줘 무시하게 해줘야겠다
- 아웃풋이 스칼라고,
- 장점은 롱레인지 디펜던시도 찾을 수 있고 , 불필요한 디펜던시는 무시할 수 있고
- 어떤거는 롱레인지하고, 어떤거는 숏레인지하고 ...

---

---

- 지난 시간이 이야기한 CNN 과 RN 의 관계를 살펴보면 아래와 같습니다.
- RN:
- 모든 다른 토큰의 관계를 봅니다. 모든 단어간의 관계를 봐서 효율적이지 못합니다.
- *h*​*t*​​=*f*(*x*​*t*​​,*x*​1​​)+⋯+*f*(*x*​*t*​​,*x*​*t*−1​​)+*f*(*x*​*t*​​,*x*​*t*+1​​)+⋯+*f*(*x*​*t*​​,*x*​*T*​​)
- CNN:
- 작은 범위의 토큰의 관계를 봅니다. 따라서 더 먼 거리의 단어간의 관계가 있을 경우 탐지할 수 없거나 더 많은 convolution 층을 쌓아야합니다.
- *h*​*t*​​=*f*(*x*​*t*​​,*x*​*t*−*k*​​)+⋯+*f*(*x*​*t*​​,*x*​*t*​​)+⋯+*f*(*x*​*t*​​,*x*​*t*+*k*​​)
- 하지만 CNN 방식을 가중치가 부여된 RN의 일종으로 볼 수도 있습니다.
- *h*​*t*​​=∑​*t*​′​​=1​*T*​​*I*(∣*t*​′​​−*t*∣≤*k*)*f*(*x*​*t*​​,*x*​*t*​′​​​​) where *I*(*S*)=1 if *S* is *True* & 0 otherwise
- 그렇다면 가중치가 0 과 1 이 아닌 그 사이의 값으로 계산 할 수 있다면 어떨까요?
- Self Attention
- *h*​*t*​​=∑​*t*​′​​=1​*T*​​*α*(*x*​*t*​​,*x*​*t*​′​​​​)*f*(*x*​*t*​​,*x*​*t*​′​​​​)
- *α*(*x*​*t*​​,*x*​*t*​′​​​​)=​∑​*t*​′​​=1​*T*​​exp(*β*(*x*​*t*​​,*x*​*t*​′​​​​))​​exp(*β*(*x*​*t*​​,*x*​*t*​′​​​​))​​
- where *β*(*x*​*t*​​,*x*​*t*​′​​​​)=*RN*(*x*​*t*​​,*x*​*t*​′​​​​)
- 장점:
- Long range & short range dependency 극복할 수 있습니다.
- 관계가 낮은 토큰은 억제하고 관계가 높은 토큰은 강조할 수 있습니다.
- 단점
- 계산 복잡도가 높고 counting 같은 특정 연산이 쉽지 않습니다.
- Recurrent Neural Network(RNN):
- 메모리를 가지고 있어서 현재까지 읽는 정보를 저장할 수 있습니다.
- 문장의 정보를 시간의 순서에 따라 압축 할 수 있습니다.
- 단점:
- 문장이 많이 길어질 수록 고정된 메모리에 압축된 정보를 담아야 하기 때문에, 앞에서 학습한 정보를 잊습니다. 이는 곧 정보의 손실을 뜻합니다.
- 토큰을 순차적으로 하나씩 읽어야 하기 때문에, 훈련 할때 속도가 기타 네트워크 보다 느립니다.
- Long Term Dependency 해결방법:
- bidirectional network를 쓰게됩니다.
- LSTM, GRU 등 RNN의 변형을 사용합니다

---

---

---

---

---

---
